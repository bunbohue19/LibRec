{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/aiotlab3/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!sh train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = pd.read_csv('../train.csv')\n",
    "valid_dataset = pd.read_csv('../val.csv')\n",
    "test_dataset = pd.read_csv('../test.csv')\n",
    "    \n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': valid_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set()\n",
    "for label_list in dataset['train']['labels']:\n",
    "    for label in label_list.split():\n",
    "        labels.add(label)\n",
    "\n",
    "for label_list in dataset['validation']['labels']:\n",
    "    for label in label_list.split():\n",
    "        labels.add(label)\n",
    "\n",
    "for label_list in dataset['test']['labels']:\n",
    "    for label in label_list.split():\n",
    "        labels.add(label)\n",
    "\n",
    "labels = sorted(list(labels))  \n",
    "id2label = {idx : label for idx, label in enumerate(labels)}\n",
    "label2id = {label : idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_one_hot(label_list, label2id):\n",
    "    num_labels = len(label2id)\n",
    "    one_hot = np.zeros(num_labels)\n",
    "    for label in label_list.split():\n",
    "        label_id = label2id[label]\n",
    "        one_hot[label_id] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import dataset\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "class MultiLabelDataset(pd.DataFrame):\n",
    "    def __init__(self, \n",
    "                 df=None, \n",
    "                 tokenizer=None, \n",
    "                 max_length=1024, \n",
    "                 max_samples=None,\n",
    "                 is_test=False\n",
    "        ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        if max_samples is not None:\n",
    "            self.df = self.df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.loc[index, \"text\"]\n",
    "        text = text.replace('</s>','<s>')\n",
    "        labels = self.df.loc[index, \"labels\"]\n",
    "        # process caption\n",
    "        model_inputs = self.tokenizer(\n",
    "            text, \n",
    "            max_length=self.max_length,  \n",
    "            truncation=True,\n",
    "        )\n",
    "        model_inputs = {k : v[0] for k,v in model_inputs.items()}\n",
    "        \n",
    "        num_labels = len(label2id)\n",
    "        one_hot = np.zeros(num_labels)\n",
    "        for label in labels.split():\n",
    "            label_id = label2id[label]\n",
    "            one_hot[label_id] = 1\n",
    "        \n",
    "        model_inputs[\"labels\"] = torch.tensor(one_hot, dtype=torch.float)\n",
    "        \n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_18223/2280212892.py\", line 1, in <module>\n",
      "    train_ds = MultiLabelDataset(\n",
      "  File \"/tmp/ipykernel_18223/3262584881.py\", line 13, in __init__\n",
      "    self.df = df.reset_index(drop=True)\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/site-packages/pandas/core/generic.py\", line 6230, in __setattr__\n",
      "    existing = getattr(self, name)\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/site-packages/pandas/core/generic.py\", line 6201, in __getattr__\n",
      "    and self._info_axis._can_hold_identifiers_and_holds_name(name)\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/site-packages/pandas/core/generic.py\", line 6201, in __getattr__\n",
      "    and self._info_axis._can_hold_identifiers_and_holds_name(name)\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/site-packages/pandas/core/generic.py\", line 6201, in __getattr__\n",
      "    and self._info_axis._can_hold_identifiers_and_holds_name(name)\n",
      "  [Previous line repeated 2960 more times]\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/site-packages/pandas/core/generic.py\", line 641, in _info_axis\n",
      "    return getattr(self, self._info_axis_name)\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/site-packages/pandas/core/generic.py\", line 6204, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "  File \"properties.pyx\", line 65, in pandas._libs.properties.AxisProperty.__get__\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/site-packages/pandas/core/generic.py\", line 6204, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "RecursionError: maximum recursion depth exceeded while calling a Python object\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1155, in get_records\n",
      "    FrameInfo(\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 780, in __init__\n",
      "    ix = inspect.getsourcelines(frame)\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/inspect.py\", line 1006, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "  File \"/home/aiotlab3/anaconda3/envs/readsum/lib/python3.9/inspect.py\", line 835, in findsource\n",
      "    raise OSError('could not get source code')\n",
      "OSError: could not get source code\n"
     ]
    }
   ],
   "source": [
    "train_ds = MultiLabelDataset(\n",
    "    df=train_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "val_ds = MultiLabelDataset(\n",
    "    df=valid_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_ds = MultiLabelDataset(\n",
    "    df=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-base\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = '../checkpoints',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name='bart-rec'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def multi_label_metrics(predictions, labels, threshold=0.2):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readsum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
